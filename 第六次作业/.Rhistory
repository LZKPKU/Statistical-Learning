L_right = matrix(nrow=n,ncol=p)
for(t in 1:M){
print(t)
W[1:n,1:p] = w[Order[1:n,1:p]]
for(j in 1:p){
L_left[1,j] = 2+2*sum(W[,j]*Y[,j])
for(i in 2:n)
L_left[i,j]=L_left[i-1,j]-Y[i-1,j]*W[i-1,j]
L_right = 4 - L_left
leftmin = min(L_left)
rightmin = min(L_right)
}
if(leftmin<rightmin){
s[t] = 1
r = which(L_left == leftmin, arr.ind = TRUE)[1,1]
c = which(L_left == leftmin, arr.ind = TRUE)[1,2]
f[t] = c
x[t] = Sorted_X[r,c]
err = leftmin/4
}
else{
s[t] = -1
r = which(L_right == rightmin, arr.ind = TRUE)[1,1]
c = which(L_right == rightmin, arr.ind = TRUE)[1,2]
f[t] = c
x[t] = Sorted_X[r,c]
err = rightmin/4
}
alpha[t] = log((1-err)/err)
# update weights
for(i in 1:n){
if(s[t] == 1){
if(X[i,f[t]]<x[t]){
if(y[i] == -1){
w[i]=w[i]*exp(alpha[t])
}
}
else{
if(y[i] == 1){
w[i]=w[i]*exp(alpha[t])
}
}
}
else{
if(X[i,f[t]]<x[t]){
if(y[i] == 1){
w[i]=w[i]*exp(alpha[t])
}
}
else{
if(y[i] == -1){
w[i]=w[i]*exp(alpha[t])
}
}
}
}
w = w/sum(w)
}
# f, s, alpha, x save the model
# Testing Process
#generating test data
Xtest = matrix(nrow=ntest,ncol=p)
ytest = vector(length=ntest)
for (i in 1:p)
Xtest[,i] = rnorm(n,0,1)
for (i in 1:ntest)
ytest[i] = sum(Xtest[i,]^2)
ytest[ytest<=9.34] = -1
ytest[ytest>9.34] = 1
# prediction function
Adaboost_predict<-function(Xtest,f,s,alpha,x){
# number of weak classifiers
M = length(x)
ntest = nrow(Xtest)
y_pred = vector(length=ntest)
for(t in 1:M){
if (s[t] == 1){
for (i in 1:ntest){
if(Xtest[i,f[t]]<x[t]){
y_pred[i] = y_pred[i] + alpha[t]*1}
else{
y_pred[i] = y_pred[i] + alpha[t]*(-1)}
}
}
else{
for (i in 1:ntest){
if(Xtest[i,f[t]]<x[t]){
y_pred[i] = y_pred[i] + alpha[t]*(-1)}
else{
y_pred[i] = y_pred[i] + alpha[t]*1}
}
}
}
y_pred[y_pred<=0] = -1
y_pred[y_pred>0] = 1
return(y_pred)
}
y_hat = Adaboost_predict(X,f,s,alpha,x)
acc_train = sum(y_hat==y)/n
y_pred = Adaboost_predict(Xtest,f,s,alpha,x)
acc = sum(y_pred==ytest)/ntest
acc_train
acc
# Parameter
p = 10;n = 10000;ntest = 10000;M=4000;
# Training Process
X = matrix(nrow=n,ncol=p)
y = vector(length=n)
# generating training data
for (i in 1:p)
X[,i] = rnorm(n,0,1)
for (i in 1:n)
y[i] = sum(X[i,]^2)
y[y<=9.34] = -1
y[y>9.34]=1
# save parameters of each classifier
f = vector(length=M)
x = vector(length=M)
s = vector(length=M)
alpha = vector(length=M)
# save weights
w = vector(length=n)
w[1:n] = rep(1/n,1)
# construct data structure
Sorted_X = matrix(nrow=n,ncol=p)
Order = matrix(nrow=n,ncol=p)
Y = matrix(nrow=n,ncol=p)
W = matrix(nrow=n,ncol=p)
for (j in 1:p)
{ #Sorted_X[i,j] stores the i-th smallest value of jth row.
#X[i,j] belongs to y[Order[i,j]]
Sorted_X[,j] = sort(X[,j])
Order[,j] = order(X[,j])
Y[1:n,j] = y[Order[1:n,j]]
}
# training enumerate each stump
L_left = matrix(nrow=n,ncol=p)
L_right = matrix(nrow=n,ncol=p)
for(t in 1:M){
print(t)
W[1:n,1:p] = w[Order[1:n,1:p]]
for(j in 1:p){
L_left[1,j] = 2+2*sum(W[,j]*Y[,j])
for(i in 2:n)
L_left[i,j]=L_left[i-1,j]-Y[i-1,j]*W[i-1,j]
L_right = 4 - L_left
leftmin = min(L_left)
rightmin = min(L_right)
}
if(leftmin<rightmin){
s[t] = 1
r = which(L_left == leftmin, arr.ind = TRUE)[1,1]
c = which(L_left == leftmin, arr.ind = TRUE)[1,2]
f[t] = c
x[t] = Sorted_X[r,c]
err = leftmin/4
}
else{
s[t] = -1
r = which(L_right == rightmin, arr.ind = TRUE)[1,1]
c = which(L_right == rightmin, arr.ind = TRUE)[1,2]
f[t] = c
x[t] = Sorted_X[r,c]
err = rightmin/4
}
alpha[t] = log((1-err)/err)
# update weights
for(i in 1:n){
if(s[t] == 1){
if(X[i,f[t]]<x[t]){
if(y[i] == -1){
w[i]=w[i]*exp(alpha[t])
}
}
else{
if(y[i] == 1){
w[i]=w[i]*exp(alpha[t])
}
}
}
else{
if(X[i,f[t]]<x[t]){
if(y[i] == 1){
w[i]=w[i]*exp(alpha[t])
}
}
else{
if(y[i] == -1){
w[i]=w[i]*exp(alpha[t])
}
}
}
}
w = w/sum(w)
}
# f, s, alpha, x save the model
# Testing Process
#generating test data
Xtest = matrix(nrow=ntest,ncol=p)
ytest = vector(length=ntest)
for (i in 1:p)
Xtest[,i] = rnorm(n,0,1)
for (i in 1:ntest)
ytest[i] = sum(Xtest[i,]^2)
ytest[ytest<=9.34] = -1
ytest[ytest>9.34] = 1
# prediction function
Adaboost_predict<-function(Xtest,f,s,alpha,x){
# number of weak classifiers
M = length(x)
ntest = nrow(Xtest)
y_pred = vector(length=ntest)
for(t in 1:M){
if (s[t] == 1){
for (i in 1:ntest){
if(Xtest[i,f[t]]<x[t]){
y_pred[i] = y_pred[i] + alpha[t]*1}
else{
y_pred[i] = y_pred[i] + alpha[t]*(-1)}
}
}
else{
for (i in 1:ntest){
if(Xtest[i,f[t]]<x[t]){
y_pred[i] = y_pred[i] + alpha[t]*(-1)}
else{
y_pred[i] = y_pred[i] + alpha[t]*1}
}
}
}
y_pred[y_pred<=0] = -1
y_pred[y_pred>0] = 1
return(y_pred)
}
y_hat = Adaboost_predict(X,f,s,alpha,x)
acc_train = sum(y_hat==y)/n
y_pred = Adaboost_predict(Xtest,f,s,alpha,x)
acc = sum(y_pred==ytest)/ntest
acc_train
acc
y[y>9.34]=1
# Parameter
p = 10;n = 10000;ntest = 10000;M=4000;
# Training Process
X = matrix(nrow=n,ncol=p)
y = vector(length=n)
# generating training data
for (i in 1:p)
X[,i] = rnorm(n,0,1)
for (i in 1:n)
y[i] = sum(X[i,]^2)
y[y<=9.34] = -1
y[y>9.34]=1
# save parameters of each classifier
f = vector(length=M)
x = vector(length=M)
s = vector(length=M)
alpha = vector(length=M)
# save weights
w = vector(length=n)
w[1:n] = rep(1/n,1)
# construct data structure
Sorted_X = matrix(nrow=n,ncol=p)
Order = matrix(nrow=n,ncol=p)
Y = matrix(nrow=n,ncol=p)
W = matrix(nrow=n,ncol=p)
for (j in 1:p)
{ #Sorted_X[i,j] stores the i-th smallest value of jth row.
#X[i,j] belongs to y[Order[i,j]]
Sorted_X[,j] = sort(X[,j])
Order[,j] = order(X[,j])
Y[1:n,j] = y[Order[1:n,j]]
}
# training enumerate each stump
# L_left means < -> 1
# L_right means < -> -1
L_left = matrix(nrow=n,ncol=p)
L_right = matrix(nrow=n,ncol=p)
for(t in 1:M){
#print(t)
W[1:n,1:p] = w[Order[1:n,1:p]]
# dynamic programming
for(j in 1:p){
L_left[1,j] = 2+2*sum(W[,j]*Y[,j])
for(i in 2:n)
L_left[i,j]=L_left[i-1,j]-Y[i-1,j]*W[i-1,j]
L_right = 4 - L_left
leftmin = min(L_left)
rightmin = min(L_right)
}
# find min error and its index, save the parameters
if(leftmin<rightmin){
s[t] = 1
r = which(L_left == leftmin, arr.ind = TRUE)[1,1]
c = which(L_left == leftmin, arr.ind = TRUE)[1,2]
f[t] = c
x[t] = Sorted_X[r,c]
err = leftmin/4
}
else{
s[t] = -1
r = which(L_right == rightmin, arr.ind = TRUE)[1,1]
c = which(L_right == rightmin, arr.ind = TRUE)[1,2]
f[t] = c
x[t] = Sorted_X[r,c]
err = rightmin/4
}
# calculate alpha
alpha[t] = log((1-err)/err)
# update weights
for(i in 1:n){
if(s[t] == 1){
if(X[i,f[t]]<x[t]){
if(y[i] == -1){
w[i]=w[i]*exp(alpha[t])
}
}
else{
if(y[i] == 1){
w[i]=w[i]*exp(alpha[t])
}
}
}
else{
if(X[i,f[t]]<x[t]){
if(y[i] == 1){
w[i]=w[i]*exp(alpha[t])
}
}
else{
if(y[i] == -1){
w[i]=w[i]*exp(alpha[t])
}
}
}
}
# normalization
w = w/sum(w)
}
# f, s, alpha, x save the model
# Testing Process
#generating test data
Xtest = matrix(nrow=ntest,ncol=p)
ytest = vector(length=ntest)
for (i in 1:p)
Xtest[,i] = rnorm(n,0,1)
for (i in 1:ntest)
ytest[i] = sum(Xtest[i,]^2)
ytest[ytest<=9.34] = -1
ytest[ytest>9.34] = 1
# prediction function
Adaboost_predict<-function(Xtest,f,s,alpha,x){
# number of weak classifiers
M = length(x)
ntest = nrow(Xtest)
y_pred = vector(length=ntest)
for(t in 1:M){
if (s[t] == 1){
for (i in 1:ntest){
if(Xtest[i,f[t]]<x[t]){
y_pred[i] = y_pred[i] + alpha[t]*1}
else{
y_pred[i] = y_pred[i] + alpha[t]*(-1)}
}
}
else{
for (i in 1:ntest){
if(Xtest[i,f[t]]<x[t]){
y_pred[i] = y_pred[i] + alpha[t]*(-1)}
else{
y_pred[i] = y_pred[i] + alpha[t]*1}
}
}
}
y_pred[y_pred<=0] = -1
y_pred[y_pred>0] = 1
return(y_pred)
}
# accuracy on training set
y_hat = Adaboost_predict(X,f,s,alpha,x)
acc_train = sum(y_hat==y)/n
# accuracy on testing set
y_pred = Adaboost_predict(Xtest,f,s,alpha,x)
acc = sum(y_pred==ytest)/ntest
acc_train
acc
library(mgcv)
library(tree)
library(prim)
library(earth)
library(gbm)
# Part I GAM
setwd("E:/My University/My Course/2017-2018第一学期/统计学习/第六次作业")
data = read.csv("spam.csv",header = FALSE)
testset = array()
trainset = array()
n = nrow(data)
set = 1:n
testset = set[data["V59"]==1]
trainset = set[data["V59"]==0]
trainx = data[,-(58:59)]
trainy = data[,58]
trainx = log(trainx+0.1)
# Part V Gradient Boosting Method
testy = trainy[testset]
trainy = trainy[trainset]
testx = trainx[testset,]
trainx = trainx[trainset,]
GBM = gbm(formula=trainy~.,data=data.frame(trainx,trainy),n.trees=3000,distribution="bernoulli",
interaction.depth=5,shrinkage = 0.01, cv.folds=3)
best.itr = gbm.perf(GBM,method="cv")
GBMpred = predict(GBM,newdata = testx,3000)
GBMpred = ifelse(GBMpred>=0.5,1,0)
accGBM = sum(as.numeric((GBMpred == testy)))/length(testset)
accGBM
GBM = gbm(formula=trainy~.,data=data.frame(trainx,trainy),n.trees=3000,distribution="bernoulli",
interaction.depth=5,shrinkage = 0.01, cv.folds=5)
best.itr = gbm.perf(GBM,method="cv")
GBMpred = predict(GBM,newdata = testx,3000)
GBMpred = ifelse(GBMpred>=0.5,1,0)
accGBM = sum(as.numeric((GBMpred == testy)))/length(testset)
accGBM
best.itr = gbm.perf(GBM,method="cv")
GBMpred = predict(GBM,newdata = testx,best.itr)
GBMpred = ifelse(GBMpred>=0.5,1,0)
accGBM = sum(as.numeric((GBMpred == testy)))/length(testset)
accGBM
library(mgcv)
library(tree)
library(prim)
library(earth)
library(gbm)
# Part I GAM
setwd("E:/My University/My Course/2017-2018第一学期/统计学习/第六次作业")
data = read.csv("spam.csv",header = FALSE)
testset = array()
trainset = array()
n = nrow(data)
set = 1:n
testset = set[data["V59"]==1]
trainset = set[data["V59"]==0]
trainx = data[,-(58:59)]
trainy = data[,58]
trainx = log(trainx+0.1)
testy = trainy[testset]
trainy = trainy[trainset]
testx = trainx[testset,]
trainx = trainx[trainset,]
V <- paste(paste("s(V",1:57,",k=4)",sep=""),collapse = '+')
form <- paste("trainy~",V,sep = "")
form = formula(form)
tic = Sys.time()
GAM = gam(formula = form,data = as.data.frame(trainx,trainy),family = "binomial")
toc = Sys.time()
toc-tic
GAMpred = predict(GAM,newdata = data.frame(testx,type="response")
GAMhat = ifelse(GAMpred>=0.5,1,0)
accGAM = sum(as.numeric((GAMhat == testy)))/length(testset)
GAMpred = predict(GAM,newdata = testx,type="response")
GAMhat = ifelse(GAMpred>=0.5,1,0)
accGAM = sum(as.numeric((GAMhat == testy)))/length(testset)
accGAM
toc-tic
1-accGAM
tic = Sys.time()
LR = glm(trainy~.,data = data.frame(trainx,trainy),family="binomial")
toc = Sys.time()
LRpred = predict(LR,newdata = data.frame(testx),type="response")
LRhat = ifelse(LRpred>=0.5,1,0)
accLR = sum(as.numeric((LRhat == testy)))/length(testset)
accLR
toc-tic
1-accLR
toc-tic
trainyy = as.factor(trainy)
testyy = as.factor(testy)
Tree = tree(trainy~.,data = data.frame(trainx,trainyy),mindev=0)
Treecv = cv.tree(Tree,FUN=prune.misclass,K=10)
Tree = tree(trainyy~.,data = data.frame(trainx,trainyy),mindev=0)
Treecv = cv.tree(Tree,FUN=prune.misclass,K=10)
size = sort(Treecv$size)[-1]
accTree = (max(Treecv$dev))/3065
for (i in size){
prune = prune.misclass(Tree,best=i)
Treepred = predict(prune,newdata = testx,type="class")
accTree = c(accTree,sum(as.numeric((Treepred != testyy)))/length(testset))
}
plot(sort(Treecv$size) ,accTree,type="b",col="green",ylim=c(0,0.5))
lines(sort(Treecv$size),sort(Treecv$dev/3065,decreasing=TRUE),type="b",col="orange")
i = which(accTree==min(accTree))
accTree[i]
1-accTree[i]
tic = Sys.time()
MARS = earth::earth(trainy~.,data = data.frame(trainx,trainy),degree=2)
toc = Sys.time()
MARSpred = predict(MARS,newdata = testx,type="class")
accMARS = sum(as.numeric((MARSpred == testy)))/length(testset)
accMARS
toc-tic
1-accMARS
toc-tic
tic = Sys.time()
GBM = gbm(trainy~.,data=data.frame(trainx,trainy),n.trees=3000,distribution="bernoulli",
interaction.depth=5,shrinkage = 0.01, cv.folds=5)
toc = Sys.time()
best.itr = gbm.perf(GBM,method="cv")
GBMpred = predict(GBM,newdata = testx,best.itr)
GBMpred = ifelse(GBMpred>=0.5,1,0)
accGBM = sum(as.numeric((GBMpred == testy)))/length(testset)
accGBM
toc-tic
best.itr
